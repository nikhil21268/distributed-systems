Implement The K-Means algorithm using grpc with python

The K-means algorithm is an iterative algorithm that partitions a dataset into K clusters. The algorithm proceeds as follows:

    Randomly initialize K cluster centroids.
    Assign each data point to the nearest cluster centroid.
    Recompute the cluster centroids based on the mean of the data points assigned to each cluster.
    Repeat steps 2 and 3 for a fixed number of iterations or until convergence (i.e., until the cluster centroids no longer change significantly).

Note that you must implement the k-means algorithm from scratch (without using any library providing K-means algorithm function).

Implementation Details:

Your implementation should include the following components:

    Master: The master program/process is responsible for running and communicating with the other components in the system. When running the master program, the following parameters should be provided as input using command line arguments:

    number of mappers (M)
    number of reducers (R)
    number of centroids (K)
    number of iterations for K-Means (Note: program should stop if the algorithm converges before)
    Other necessary information if required (This should be reasonable. Please check with us - if you are not sure!)

    Input Split (invoked by master): You will need to write code to divide the input data (single file) into smaller chunks that can be processed in parallel by multiple mappers. Your code does not need to produce separate input files for each mapper, but each mapper should process a different chunk of the file input data.
    For partitioning the input data across different mappers:
Input data contains only one big file. Each mapper reads the entire input file and then processes only the indices allocated to it by the master

the master does not distribute the actual data to the mappers as that is going to be unnecessary network traffic.

Master should try to split the data equally (or almost equally) based on the number of mappers.


    Map (invoked by mapper):

    You will need to write code to apply the Map function to each input split to generate intermediate key-value pairs.
    Mapper should read the input split by itself (based on the information provided by the master). Master should not send the input data points to the mapper.
    Inputs to the Map function: 

    Input split assigned by the master: 

    This split contains a list of data points
    Range of Indices

    List of Centroids from the previous iteration
    Other necessary information if required (This should be reasonable. Please check with us - if you are not sure!)

    Output from the Map function: For each data point processed by the map function, the function outputs: 

    Key: index of the nearest centroid to which the data point belongs
    Value: value of the data point itself.

    The output of each Map function should be written to a file in the mapper’s directory on the local file system. 
    The output of each Map function should be passed to the partition function which will then write the output in a partition file inside the mapper’s directory on the local file system. (Look at the directory structure given below)
    Note that each mapper needs to run as a separate process.
Please ensure that mappers run in parallel, not sequentially. 

    Partition (invoked by mapper): 

    The output of the Map function (as mentioned in the mapper above) needs to be partitioned into a set of smaller partitions.
    In this step, you will write a function that takes the list of key-value pairs generated by the Map function and partitions them into smaller partitions.
    The partitioning function should ensure that

    all key-value pairs belonging to the same key are sent to the same partition file.
    distribute the different keys equally (or almost equally) among each of the partitions. This can be done using very simple and reasonable partition functions ( such as key % num_reducers)

    Each partition file is picked up by a specific reducer during shuffling and sorting.
    If there are M mappers and R reducers, each mapper should have R file partitions. This means that there will be M*R partitions in total.
    This step is performed by the mapper.

    Shuffle and sort (invoked by reducer): 

    You must write code to sort the intermediate key-value pairs by key and group the values that belong to the same key.
    This is typically done by sending the intermediate key-value pairs to the reducers based on the key.
    This step is performed by the reducer.

    Reduce (invoked by reducer): 

    The reducer will receive the intermediate key-value pairs from the mapper, perform the shuffle & sort function as mentioned, and produce a set of final key-value pairs as output.
    You will need to write code to apply the Reduce function to each group of values that belong to the same key to generate the final output.
    Input to the reduce function:

    Key: Centroid id
    Value: List of all the data points which belong to this centroid id (this information is available after shuffle and sorting)
    Other necessary information (This should be reasonable. Please check with us - if you are not sure!)

    Output of the reduce function:

    Key: Centroid Id
    Value: Updated Centroid

    The output of each Reduce function should be written to a file in the reducer’s directory on the local file system.
    Note that each reducer needs to run as a separate process.
Please ensure that once the mappers have finished, all the reducers run in parallel, not sequentially.

    Centroid Compilation (invoked by master): The master needs to parse the output generated by all the reducers to compile the final list of (K) centroids and store them in a single file. This list of centroids is considered as input for the next iteration. Before the first iteration, the centroids should be randomly selected from the input data points.

    Master must use gRPC calls to contact the Reducer for reading the output files (since in practice, master and reducer are not running on the same machines)

Consider below points.txt file for the datapoints:
0.4,7.2
0.8,9.8
-1.5,7.3
8.1,3.4
7.3,2.3
9.1,3.1
8.9,0.2
11.5,-1.9
10.2,0.5
9.8,1.2
8.5,2.7
10.3,-0.3
9.7,0.8
8.3,2.9
0.0,7.1
0.9,9.6
-1.6,7.4
0.3,7.2
0.7,9.9
-1.7,7.5
0.5,7.4
0.9,9.7
-1.8,7.6
11.1,-1.5
10.8,-0.6
9.5,1.5
8.7,2.4
11.2,-1.2
10.5,-0.1
9.3,1.9
8.6,2.6

also consider below centroids.txt file for final centroids:
-0.17500000000000002,8.141666666666667
9.547368421052632,1.0473684210526315

Print Statements:

Please log/print everything in a dump.txt file (just like we did in assignment 2) for easy debugging/monitoring.

Print/Display the following data while executing the master program for each iteration:
    Iteration number
    Execution of gRPC calls to Mappers or Reducers
    gRPC responses for each Mapper and Reducer function (SUCCESS/FAILURE)
    Centroids generated after each iteration (including the randomly initialized centroids)