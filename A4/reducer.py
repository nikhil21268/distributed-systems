import grpc
import mapReduce_pb2
import mapReduce_pb2_grpc
import random
import numpy as np
from concurrent import futures
import argparse
import os

random.seed(0)

'''
    Shuffle and sort (invoked by reducer): 

    You must write code to sort the intermediate key-value pairs by key and group the values that belong to the same key.
    This is typically done by sending the intermediate key-value pairs to the reducers based on the key.
    This step is performed by the reducer.

    Reduce (invoked by reducer): 

    The reducer will receive the intermediate key-value pairs from the mapper, perform the shuffle & sort function as mentioned, and produce a set of final key-value pairs as output.
    You will need to write code to apply the Reduce function to each group of values that belong to the same key to generate the final output.
    Input to the reduce function:

    Key: Centroid id
    Value: List of all the data points which belong to this centroid id (this information is available after shuffle and sorting)
    Other necessary information (This should be reasonable. Please check with us - if you are not sure!)

    Output of the reduce function:

    Key: Centroid Id
    Value: Updated Centroid

    The output of each Reduce function should be written to a file in the reducerâ€™s directory on the local file system.
    Note that each reducer needs to run as a separate process.
Please ensure that once the mappers have finished, all the reducers run in parallel, not sequentially.

    Centroid Compilation (invoked by master): The master needs to parse the output generated by all the reducers to compile the final list of (K) centroids and store them in a single file. This list of centroids is considered as input for the next iteration. Before the first iteration, the centroids should be randomly selected from the input data points.

    Master must use gRPC calls to contact the Reducer for reading the output files (since in practice, master and reducer are not running on the same machines)
'''

class Reducer(mapReduce_pb2_grpc.MapReduceServicer):
    id= 0
    
    def Reduce(self, request, context):
        print("\nReceived Reduce request")
        self.id = request.id
        mapper_nodes = eval(request.mapper_nodes)
        
        # shuffle and sort
        entries = self.shuffle_and_sort(mapper_nodes)
        print("\nShuffle and Sort Task Completed: ", entries)

        # group the values that belong to the same key and sorting implicitly using dictionary grouped_entries
        grouped_entries = {}
        for key, value in entries:
            if key not in grouped_entries:
                grouped_entries[key] = []
            grouped_entries[key].append(value)

        # reduce
        updated_centroids =  self.reduce(grouped_entries)
        print("\nUpdated centroids: ", updated_centroids)
        print("Reduce Task Completed")
        return mapReduce_pb2.ReduceResponse(success= True, centroids=str(updated_centroids))

    def shuffle_and_sort(self, mapper_nodes):
        entries = []
        # send a get entries request to the mappers
        for i in range(len(mapper_nodes)):
            channel = grpc.insecure_channel(mapper_nodes[i])
            stub = mapReduce_pb2_grpc.MapReduceStub(channel)
            response = stub.GetEntries(mapReduce_pb2.GetEntriesRequest(id=self.id))
            if response:
                if response.entries:
                    print(f"\nPartitions found from mapper {i}:")
                    print(response.entries)
                    entries.extend(eval(response.entries))
                else:
                    print("No entries found")
            else:
                print("No response from mapper")
        # return entries.sort(key=lambda x: x[0])
        return entries

    def reduce(self, entries):
        # reduce function
        # calculate the updated centroids
        updated_centroids = []
        for key in entries:
            values = entries[key]
            updated_centroid = tuple(np.mean(values, axis=0))
            updated_centroids.append((key, tuple(updated_centroid)))

        # write the updated centroids to a file
        # create a directory for the reducer if it doesn't exist
        os.makedirs(f'Reducers', exist_ok=True)
        with open(f'Reducers/R{self.id}.txt', 'w') as f:
            for centroid in updated_centroids:
                f.write(f'{centroid}\n')

        # return the updated centroids
        return updated_centroids

def parse_args():
    parser = argparse.ArgumentParser(description='Reducer')
    parser.add_argument('--port', type=int, help='Port of the reducer node')
    args = parser.parse_args()
    return args

if __name__ == "__main__":
    args = parse_args()
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    mapReduce_pb2_grpc.add_MapReduceServicer_to_server(Reducer(), server)
    server.add_insecure_port(f'localhost:{args.port}')
    server.start()
    server.wait_for_termination()
