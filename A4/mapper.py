# this is the mapper node for the map-reduce system
# read todo.txt for more information on implementation

import grpc
import mapReduce_pb2
import mapReduce_pb2_grpc
import random
import numpy as np
from concurrent import futures
import argparse
import os

random.seed(0)

'''
Mapper should read the input split by itself (based on the information provided by the master). Master should not send the input data points to the mapper.
    Inputs to the Map function: 

    Input split assigned by the master: 

    This split contains a list of data points
    Range of Indices

    List of Centroids from the previous iteration
    Other necessary information if required (This should be reasonable. Please check with us - if you are not sure!)

    Output from the Map function: For each data point processed by the map function, the function outputs: 

    Key: index of the nearest centroid to which the data point belongs
    Value: value of the data point itself.

    The output of each Map function should be written to a file in the mapper’s directory on the local file system. 
    The output of each Map function should be passed to the partition function which will then write the output in a partition file inside the mapper’s directory on the local file system. (Look at the directory structure given below)
    Note that each mapper needs to run as a separate process.
Please ensure that mappers run in parallel, not sequentially. 

    Partition (invoked by mapper): 

    The output of the Map function (as mentioned in the mapper above) needs to be partitioned into a set of smaller partitions.
    In this step, you will write a function that takes the list of key-value pairs generated by the Map function and partitions them into smaller partitions.
    The partitioning function should ensure that

    all key-value pairs belonging to the same key are sent to the same partition file.
    distribute the different keys equally (or almost equally) among each of the partitions. This can be done using very simple and reasonable partition functions ( such as key % num_reducers)

    Each partition file is picked up by a specific reducer during shuffling and sorting.
    If there are M mappers and R reducers, each mapper should have R file partitions. This means that there will be M*R partitions in total.
    This step is performed by the mapper.'''

class Mapper(mapReduce_pb2_grpc.MapReduceServicer):
    id = 0

    def Map(self, request, context):
        # take grpc request and extract the data points and centroids
        print("\nReceived Map request")
        print(f"start: {request.start}, end: {request.end}, centroids: {request.centroids}, r: {request.r}, id: {request.id}")

        self.id = request.id
        start = request.start
        end = request.end
        centroids = eval(request.centroids)

        # read the data points from the file
        with open('points.txt', 'r') as f:
            points = f.readlines()
            points = points[start:end]

        # calculate the nearest centroid for each point
        output = []
        for point in points:
            point = eval(point)
            # find the nearest centroid
            nearest_centroid = min(centroids, key=lambda x: np.linalg.norm(np.array(x) - np.array(point)))
            output.append((nearest_centroid, point))

        print("Output: ", output)
        self.partition(output, request.r)

        print("Map request completed")
        return mapReduce_pb2.MapResponse(success=True)
    
    def partition(self, output, R):
        print("\nPartitioning output")
        # partition the output into R partitions
        
        partitioned_output = [[] for _ in range(R)]
        for key, value in output:
            partitioned_output[(round(key[0])+round(key[1])) % R].append((key, value))
        # print(partitioned_output)

        # write the partitioned output to files
        # create a directory for the mapper if it doesn't exist
        os.makedirs(f'Mappers/M{self.id}', exist_ok=True)

        for i in range(R):
            with open(f'Mappers/M{self.id}/partition_{i}.txt', 'w') as f:
                for key, value in partitioned_output[i]:
                    f.write(f'{key},{value}\n')

        print("Partitioning completed\n")

    def GetEntries(self, request, context):
        # read the partition file
        print(f"\nReceived GetEntries request from Reducer {request.id}")
        with open(f'Mappers/M{self.id}/partition_{request.id}.txt', 'r') as f:
            entries = f.readlines()
            entries = [entry.strip() for entry in entries]
            entries = [eval(entry) for entry in entries]
        
        print("entries: ", entries)
        print("GetEntries request completed")
        return mapReduce_pb2.GetEntriesResponse(entries=str(entries))

def parse_args():
    parser = argparse.ArgumentParser(description='Mapper node for the map-reduce system')
    parser.add_argument('--port', type=int, help='Port of the mapper node')
    return parser.parse_args()
    
if __name__ == "__main__":
    args = parse_args()
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    mapReduce_pb2_grpc.add_MapReduceServicer_to_server(Mapper(), server)
    server.add_insecure_port(f'localhost:{args.port}')
    server.start()
    server.wait_for_termination()
